<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Chenjie Cao</title>
  
  <meta name="author" content="Chenjie Cao">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<span id="busuanzi_container_site_uv">Visitors: <span id="busuanzi_value_site_uv"></span></span>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Chenjie Cao ÊõπËæ∞Êç∑</name>
              </p>
              <p>Welcome! I am currently a fourth-year Ph.D. student under the supervision of <a href="https://yanweifu.github.io/">Prof. Yanwei Fu</a> from Fudan University.
                My research interests focus on computer vision, which includes <strong>image inpainting</strong>, <strong>image synthesis</strong>, <strong>multi-view stereo</strong>, neural surface reconstruction, and feature matching.
              </p>
              <p style="text-align:center">
                <a href="ccjdurandal422@163.com">Email</a> &nbsp/&nbsp
                <a href="data/resume_ccj.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=1INK-I0AAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/ewrfcas/">GitHub</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="img/profile.jpg"><img style="width:65%;max-width:65%" alt="profile photo" src="img/profile.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Education</heading>
              <p>
                * Ph.D of Statistics (Machine Learning track), Fudan University, 2020 - Present.
              </p>
              <p>
                * Master of Computer Science and Technology, East China University of Science and Technology (ECUST), 2016 - 2019.
              </p>
              <p>
                * Bachelor of Computer Science and Technology, East China University of Science and Technology (ECUST), 2012 - 2016.
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <p align="left:100px">* means equal contribution</p>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='img/ARCI_teaser.jpg' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2305.11577">
                <papertitle>Harnessing Text-to-Image Attention Prior for Reference-based Multi-view Image Synthesis.</papertitle>
              </a>
              <br>
              <strong>Chenjie Cao</strong>,
              Yunuo Cai,
              Qiaole Dong,
              Yikai Wang,
              Yanwei Fu
              <br>
              <em>Preprint</em>
              <br>
              <a href="https://ewrfcas.github.io/ARCI/">page</a> /
              <a href="https://arxiv.org/abs/2305.11577">paper</a> /
              <a href="https://github.com/ewrfcas/ARCI">code</a>
              <p></p>
              <p>
                We formulate the reference-guided multi-view synthesis as an contextual inpainting issue, which is addressed by
                pre-existing attention modules of pre-trained Text-to-Image model through prompt tuning.
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='img/SiameseNet.jpg' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2308.03217.pdf">
                <papertitle>Local Consensus Enhanced Siamese Network with Reciprocal Loss for Two-view Correspondence Learning.</papertitle>
              </a>
              <br>
              Linbo Wang,
              Jing Wu,
              Xianyong Fang,
              Zhengyi Liu,
              <strong>Chenjie Cao</strong>,
              Yanwei Fu
              <br>
              <em>ACMMM 2023</em>
              <br>
                  <a href="https://arxiv.org/pdf/2308.03217.pdf">paper</a>
              <p></p>
              <p>
                A Siamese network with a reciprocal loss is proposed to classify inliers/outliers for two-view matching.
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='img/cascadematching.jpg' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2303.02885">
                <papertitle>Improving Transformer-based Image Matching by Cascaded Capturing Spatially Informative Keypoints.</papertitle>
              </a>
              <br>
              <strong>Chenjie Cao</strong>,
              Yanwei Fu
              <br>
              <em>ICCV 2023</em>
              <br>
                  <a href="https://ewrfcas.github.io/CasMTR/">page</a> /
                  <a href="https://arxiv.org/abs/2303.02885">paper</a> /
                  <a href="https://github.com/ewrfcas/CasMTR">code</a>
              <p></p>
              <p>
                Matching pairs located in spatially informative keypoints of both reference and target views enjoy
                better pose estimation. Thus a transformer-based cascaded matching model and a simple yet effective NMS filter are proposed.
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='img/zits++.jpg' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2210.05950">
                <papertitle>ZITS++: Image Inpainting by Improving the Incremental Transformer on Structural Priors.</papertitle>
              </a>
              <br>
              <strong>Chenjie Cao*</strong>,
              Qiaole Dong*,
              Yanwei Fu
              <br>
              <em>TPAMI 2023</em>
              <br>
              <a href="https://ewrfcas.github.io/ZITS-PlusPlus/">page</a> /
              <a href="https://arxiv.org/abs/2210.05950">paper</a> /
              <a href="https://github.com/ewrfcas/ZITS-PlusPlus">code</a>
              <p></p>
              <p>
                The extension of CVPR work --ZITS. We further discuss the influence of various image priors
                on inpainting, and choice to use learning-based edges (L-Edge) as the new prior to enhance the meaningful structure recovery.
              </p>
            </td>
          </tr>

          <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
              <img src='img/matchflow.jpg' width="160">
            </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <a href="https://arxiv.org/abs/2303.08384">
              <papertitle>Rethinking Optical Flow from Geometric Matching Consistent Perspective.</papertitle>
            </a>
            <br>
            Qiaole Dong*,
            <strong>Chenjie Cao*</strong>,
            Yanwei Fu
            <br>
            <em>CVPR 2023</em>
            <br>
            <a href="https://dqiaole.github.io/MatchFlow/">page</a> /
            <a href="https://arxiv.org/abs/2303.08384">paper</a> /
            <a href="https://github.com/DQiaole/MatchFlow">code</a>
            <p></p>
            <p>
              Improving the optical flow estimation through the image matching pre-text task.
            </p>
          </td>
        </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='img/mvsformer.jpg' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2208.02541">
                <papertitle>MVSFormer: Multi-View Stereo by Learning Robust Image Features and Temperature-based Depth.</papertitle>
              </a>
              <br>
              <strong>Chenjie Cao</strong>,
              Xinlin Ren,
              Yanwei Fu
              <br>
              <em>TMLR, 2023</em>
              <br>
                  <a href="https://openreview.net/forum?id=2VWR6JfwNo">openreview</a> /
                  <a href="https://arxiv.org/abs/2208.02541">paper</a> /
                  <a href="https://github.com/ewrfcas/MVSFormer">code</a>
              <p></p>
              <p>
                We explore the usage of pre-trained ViTs for multi-view stereo, and further propose to unify both
                classification and regression-based depth predictions. MVSFormer achieves 1-st place in <a href="https://www.tanksandtemples.org/">Tanks-and-Temple</a>, and achieve 2-nd place in <a href="https://gigavision.cn/">GigaMVS</a>2022.
              </p>
            </td>
          </tr>
					
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='img/mae-far.jpg' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2208.01837">
                <papertitle>Learning Prior Feature and Attention Enhanced Image Inpainting.</papertitle>
              </a>
              <br>
              <strong>Chenjie Cao*</strong>,
              Qiaole Dong,
              Yanwei Fu
              <br>
              <em>ECCV, 2022</em>
              <br>
                <a href="https://ewrfcas.github.io/MAE-FAR/">page</a> /
                <a href="https://arxiv.org/abs/2208.01837">paper</a> /
                <a href="https://github.com/ewrfcas/MAE-FAR">code</a>
              <p></p>
              <p>Improving the image inpainting with Masked AutoEncoder pre-training and attention-based restoration.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='img/zits.jpg' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Dong_Incremental_Transformer_Structure_Enhanced_Image_Inpainting_With_Masking_Positional_Encoding_CVPR_2022_paper.pdf">
                <papertitle>Incremental Transformer Structure Enhanced Image Inpainting with Masking Positional Encoding.</papertitle>
              </a>
              <br>
              Qiaole Dong*,
              <strong>Chenjie Cao*</strong>,
              Yanwei Fu
              <br>
              <em>CVPR, 2022</em>
              <br>
                <a href="https://dqiaole.github.io/ZITS_inpainting/">page</a> /
                <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Dong_Incremental_Transformer_Structure_Enhanced_Image_Inpainting_With_Masking_Positional_Encoding_CVPR_2022_paper.pdf">paper</a> /
                <a href="https://github.com/DQiaole/ZITS_inpainting">code</a>
              <p></p>
              <p>Learning structural priors (lines, edges) with transformers at first, then recovering textures with FFC based CNNs.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='img/wavelet.jpg' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2206.03113">
                <papertitle>Wavelet Prior Attention Learning in Axial Inpainting Network.</papertitle>
              </a>
              <br>
              <strong>Chenjie Cao</strong>,
              Chengrong Wang,
              Yuntao Zhang,
              Yanwei Fu
              <br>
              <em>Preprint</em>
              <br>
                <a href="https://arxiv.org/abs/2206.03113">paper</a>
              <p></p>
              <p>Firstly introducing wavelet prior for image inpainting. We also use axial-transformer to enhance the face/structural recovery.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='img/p2m++.jpg' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2204.09866">
                <papertitle>Pixel2mesh++: 3d Mesh Generation and Refinement from Multi-View Images.</papertitle>
              </a>
              <br>
              Chao Wen*,
              Yinda Zhang*,
              <strong>Chenjie Cao</strong>,
              Zhuwen Li,
              Xiangyang Xue,
              Yanwei Fu
              <br>
              <em>TPAMI, 2022</em>
              <br>
                <a href="https://ewrfcas.github.io/Pixel2MeshPlusPlus-MVDISN">page</a> /
                <a href="https://arxiv.org/abs/2204.09866">paper</a> /
                <a href="https://github.com/ewrfcas/Pixel2MeshPlusPlus-MVDISN">code</a>
              <p></p>
              <p>The extension of P2M++, which makes P2M++ also work on SDF-based methods.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='img/face.jpg' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/document/9747428/">
                <papertitle>High-Fidelity Portrait Editing via Exploring Differentiable Guided Sketches from the Latent Space.</papertitle>
              </a>
              <br>
              Chengrong Wang,
              <strong>Chenjie Cao</strong>,
              Yanwei Fu,
              Xiangyang Xue
              <br>
              <em>ICASSP, 2022</em>
              <br>
                <a href="https://ieeexplore.ieee.org/document/9747428/">paper</a>
              <p></p>
              <p>Leveraging GAN inversion and differentiable edge detector to achieve an effective face editing.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='img/ilat.jpg' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://papers.nips.cc/paper_files/paper/2021/file/9996535e07258a7bbfd8b132435c5962-Paper.pdf">
                <papertitle>The Image Local Autoregressive Transformer.</papertitle>
              </a>
              <br>
              <strong>Chenjie Cao</strong>,
              Yuxin Hong,
              Xiang Li,
              Chengrong Wang,
              Chengming Xu,
              XiangYang Xue,
              Yanwei Fu
              <br>
              <em>NeurIPS, 2021</em>
              <br>
                <a href="https://papers.nips.cc/paper_files/paper/2021/file/9996535e07258a7bbfd8b132435c5962-Paper.pdf">paper</a> /
                <a href="https://github.com/ewrfcas/iLAT">code</a>
              <p></p>
              <p>Propose the local autoregressive for local editing. The two-stream convolution is proposed to tackle information leakage.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='img/mst.jpg' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2103.15087">
                <papertitle>Learning a Sketch Tensor Space for Image Inpainting of Man-Made Scenes.</papertitle>
              </a>
              <br>
              <strong>Chenjie Cao</strong>,
              Yanwei Fu
              <br>
              <em>ICCV, 2021</em>
              <br>
                <a href="https://ewrfcas.github.io/MST_inpainting/">page</a> /
                <a href="https://arxiv.org/abs/2103.15087">paper</a> /
                <a href="https://github.com/ewrfcas/MST_inpainting">code</a>
              <p></p>
              <p>We firstly introduce segment lines to improve the image inpainting.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='img/clue.jpg' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://aclanthology.org/2020.coling-main.419/">
                <papertitle>CLUE: A Chinese Language Understanding Evaluation Benchmark.</papertitle>
              </a>
              <br>
              Liang Xu,
              Xuanwei Zhang,
              Lu Li,
              Hai Hu,
              <strong>Chenjie Cao</strong>,
              Yudong Li, Yechen Xu, Kai Sun,
              Dian Yu, Cong Yu, Yin Tian, Qianqian Dong, Weitang Liu, Bo Shi, Yiming Cui, Junyi Li,
              Jun Zeng, Rongzhao Wang, Weijian Xie, Yanting Li, Yina Patterson, Zuoyu Tian,
              Yiwen Zhang, He Zhou, Shaoweihua Liu, Zhe Zhao, Qipeng Zhao, Cong Yue,
              Xinrui Zhang, Zhengliang Yang, Kyle Richardson, Zhenzhong Lan
              <br>
              <em>COLING, 2020</em>
              <br>
                <a href="https://www.cluebenchmarks.com/">page</a> /
                <a href="https://aclanthology.org/2020.coling-main.419/">paper</a> /
                <a href="https://github.com/CLUEbenchmark/CLUE">code</a>
              <p></p>
              <p>A comprehensive Chinese NLP benchmark.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='img/tnnls.jpg' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/document/8968753">
                <papertitle>Entropy and Confidence-based Undersampling Boosting Random Forests for Imbalanced Problems.</papertitle>
              </a>
              <br>
              Zhe Wang,
              <strong>Chenjie Cao</strong>,
              Yujin Zhu
              <br>
              <em>TNNLS, 2020</em>
              <br>
                <a href="https://ieeexplore.ieee.org/document/8968753">paper</a>
              <p></p>
              <p>My early work about using ensemble learning to address imbalanced problems.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='img/sibert.jpg' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://aclanthology.org/2020.lrec-1.293.pdf">
                <papertitle>SiBert: Enhanced Chinese Pre-trained Language Model with Sentence Insertion.</papertitle>
              </a>
              <br>
              Jiahao Chen,
              <strong>Chenjie Cao</strong>,
              Xiuyan Jiang
              <br>
              <em>LREC, 2020</em>
              <br>
                <a href="https://aclanthology.org/2020.lrec-1.293.pdf">paper</a>
              <p></p>
              <p>Improving the BERT pre-training with Sentence Insertion. SiBert won 1-st of CMRC2019.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='img/nn.jpg' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://www.sciencedirect.com/science/article/abs/pii/S0893608019301765">
                <papertitle>Cascade Interpolation Learning with Double Subspaces and Confidence Disturbance for Imbalanced Problems.</papertitle>
              </a>
              <br>
              Zhe Wang,
              <strong>Chenjie Cao</strong>
              <br>
              <em>NN, 2019</em>
              <br>
                <a href="https://www.sciencedirect.com/science/article/abs/pii/S0893608019301765">paper</a>
              <p></p>
              <p>My early work about using ensemble learning to address imbalanced problems.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='img/kbs.jpg' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://www.sciencedirect.com/science/article/abs/pii/S0950705118300947">
                <papertitle>IMCStacking: Cost-Sensitive Stacking Learning with Feature Inverse Mapping for Imbalanced Problems.</papertitle>
              </a>
              <br>
              <strong>Chenjie Cao</strong>
              Zhe Wang
              <br>
              <em>KBS, 2018</em>
              <br>
                <a href="https://www.sciencedirect.com/science/article/abs/pii/S0950705118300947">paper</a>
              <p></p>
              <p>My early work about using ensemble learning to address imbalanced problems.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='img/asc.jpg' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://www.sciencedirect.com/science/article/abs/pii/S1568494618301121">
                <papertitle>Information Entropy based Sample Reduction for Support Vector Data Description.</papertitle>
              </a>
              <br>
              Li DongDong, Zhe Wang,
              <strong>Chenjie Cao</strong>,
              Yu Liu
              <br>
              <em>ASC, 2018</em>
              <br>
                <a href="https://www.sciencedirect.com/science/article/abs/pii/S1568494618301121">paper</a>
              <p></p>
              <p>My early work about using SVDD to address imbalanced problems.</p>
            </td>
          </tr>


        </tbody></table>

				
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Misc</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>

          <tr>
            <td width="75%" valign="center">
              <strong>Second-Place</strong> in <a href="https://gigavision.cn/">GigaReconstruction</a> 2022.
            </td>
          </tr>

          <tr>
            <td width="75%" valign="center">
              <strong>Reviewer</strong> in TPAMI, IJCV, ACMMM2023, CVPR2023, NeurIPS 2022,2023, ICML2023, ICLR2023.
            </td>
          </tr>
          <tr>
            <td width="75%" valign="center">
              Yanwei Fu, Shenghua Gao, <strong>Chenjie Cao</strong> and Qiaole Dong. Tutorial: <a href="https://dqiaole.github.io/priors_guided_image_editing_synthesis/">The Priors Guided Image Editing and Synthesis</a> in ACCV2022.
            </td>
          </tr>

					
					
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                <a href="https://jonbarron.info/">Template</a>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>

